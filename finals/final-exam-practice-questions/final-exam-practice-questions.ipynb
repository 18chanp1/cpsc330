{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951d7d7a-6c1c-4bb4-8361-7540f8aa37b9",
   "metadata": {},
   "source": [
    "# CPSC 330 practice questions\n",
    "\n",
    "The University of British Columbia\n",
    "\n",
    "DO NOT DISTRIBUTE WITHOUT PERMISSION\n",
    "\n",
    "**These sample questions are designed to give you an understanding of the exam format, not to serve as comprehensive coverage of all the topics we've discussed. They also do not reflect the total number of questions that will be on the actual exam. As you study, please do not depend only on these questions or memorize their answers. There are no shortcuts to effectively prepare for the exam. It's essential to review all the material and ensure you have a solid understanding of the key concepts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e419689-0479-4794-a356-f2270feba7e3",
   "metadata": {},
   "source": [
    "## Q1 \n",
    "---\n",
    "The training accuracy of `DummyClassifier`  with `strategy=\"most_frequent\"` might change depending upon how you split the data.\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "    [X] True\n",
    "    [ ] False\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b94eee8-8384-4d9e-ad06-83038f4d3d41",
   "metadata": {},
   "source": [
    "## Q2\n",
    "---\n",
    "Is it a good practice to treat the `random_state` of `train_test_split` as a hyperparameter and pick the `random_state` which gives you the best cross-validation scores? Briefly explain.\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "> No it's not a good practice. If different random states give different results, it means that the model is very much sensitive to the training data it is trained on. If we pick the random states which gives the best cross-validation score the results might be too optimistic and the model is less likely to generalize on unseen data.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e347e7-7b90-44c3-bc28-45bde8b6ad10",
   "metadata": {},
   "source": [
    "## Q3\n",
    "---\n",
    "In Lecture 3 we talked about different splits: train, validation, test, deployment. What is the difference between test data and deployment data?\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "> Test data has targets whereas deployment data doesn't have targets.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4dc6f-18e5-47e1-8712-fda0393ec352",
   "metadata": {},
   "source": [
    "## Q4\n",
    "---\n",
    "![](img/trainvalid.png)\n",
    "\n",
    "Explain how the plot above illustrates the fundamental tradeoff by filling in the missing words.\n",
    "\n",
    "As you increase the model complexity, the training accuracy tends to go `[trainacc]` and the gap between training and validation accuracy tends to go `[gap]`.\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "> `up` for `[trainacc]` and `up` for `[gap]`. As we increase complexity (C), the training score goes up, but the train-test gap also goes up - this is the fundamental tradeoff.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea4445-6912-4ae7-9e43-f624c3fab1b4",
   "metadata": {},
   "source": [
    "## Q5\n",
    "---\n",
    "Give one similarity and one difference between KNNs and RBF SVMs.\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    ">- Similarity: Both are analogy-based models.\n",
    ">- Differences: (Many possible answers)\n",
    ">    * During prediction KNNs find distance of the query point with all examples whereas RBF SVMs find key examples called support vectors and it calculates a weighted distance of the query point to each support vector.\n",
    ">    * KNN has an integer hyperparameter `k` (`n_neighbors` in sklearn) whereas RBF SVMs have two main continuous hyperparameters: `C` and `gamma`.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e43bed-b0ef-44e4-9245-b7d39e53dbb3",
   "metadata": {},
   "source": [
    "## Q6\n",
    "---\n",
    "One of the imputation approaches we talked about was\n",
    "\n",
    "```SimpleImputer(strategy='constant', fill_value='?')```\n",
    "\n",
    "This means that every missing value gets replaced with a '?'. Let's say we use this approach on a categorical column, which is then passed into one-hot encoding. Thus, the '?' category would get its own column in the encoded dataset, and thus its own feature importance. Let's say we observe a high feature importance for the '?' column on our final model, and yet we happen to know that in deployment we will never have missing values for this feature. What are the implications of this information on our model's performance in deployment? Briefly explain.\n",
    "\n",
    "If it's helpful, you can assume the model is not overfit; that is, your training, cross-validation, and test scores are all about the same.\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "In deployment the '?' column will always take value 0, so this feature will basically be ignored. Since it was important, we can expect worse performance in deployment. (If the model was overfitting on this feature then maybe \"removing\" it could have actually helped though - hard to say.)\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68253eb1-e1e0-4021-bdb4-7a3fc4f3b23a",
   "metadata": {},
   "source": [
    "## Q7\n",
    "---\n",
    "Suppose you are working on the problem of comment moderation, where the target is whether to accept or reject a given comment. Below are some features given to you.\n",
    "\n",
    "| comment_text                                                                         | toxicity_severity | comment_type | is_threatening | target |\n",
    "|--------------------------------------------------------------------------------------|-------------------|--------------|----------------|--------|\n",
    "| Wars after wars! What a peaceful politician he turned out to be!                     | mild              | sarcastic    | 0              | accept |\n",
    "| That's stupid. Stop behaving like a 5-year old.                                      | moderate          | insulting    | 0              | reject |\n",
    "| You are my hero!                                                                     | none              | supportive   | 0              | accept |\n",
    "| You hit the right notes in this piece.                                               | none              | supportive   | 0              | accept |\n",
    "| This person shouldn't be allowed to talk or write.  Otherwise we will attack them.   | severe            | hate speech  | 1              | reject |\n",
    "\n",
    "What encoding would you apply for each of the features above: `comment_text`, `toxicity_severity`, `comment_type`, and `is_threatening`?\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "> |                   |                       |\n",
    "> |-------------------|-----------------------|\n",
    "> | `comment_text`      | Bag of words encoding |\n",
    "> | `toxicity_severity` | Ordinal encoding      |\n",
    "> | `comment_type`      | One-hot encoding      |\n",
    "> | `is_threatening`    | None                  |\n",
    "> \n",
    "> Other Incorrect Match Options:\n",
    "> - Scaling\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76478e30-9f6c-4e22-af9c-766560265e73",
   "metadata": {},
   "source": [
    "## Q8\n",
    "---\n",
    "In Lecture 6 we discussed encoding categorical variables. Imagine we had a feature in our census dataset called `bank_account_size_category` representing the size of the person's bank account, and imagine that running\n",
    "```python\n",
    "df_train[\"bank_account_size_category\"].value_counts()\n",
    "```\n",
    "returns the following:\n",
    "```python\n",
    "tiny     5\n",
    "small    100\n",
    "medium   15000\n",
    "large    10000\n",
    "enormous 2\n",
    "```\n",
    "Suppose you decide to use one-hot encoding on this column. What would be the problem when it comes to making predictions for people with tiny or enormous bank accounts? What encoding would be more appropriate for this column?\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "> Because there is very little data for tiny and enormous bank account size. Thus OHE will have almost nothing to learn with. Ordinal encoding will be more appropriate for this column. \n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa2938-77e8-4435-a8da-1670277947b8",
   "metadata": {},
   "source": [
    "## Q9\n",
    "---\n",
    "Suppose you are tuning 8 hyperparameters, each taking 10 values, using `RandomizedSearchCV` with `CV=10` and `n_iter=80`. How many 10-fold cross-validations would be carried out in this case?\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "    [ ] 10^8\n",
    "    [ ] 8^10\n",
    "    [X] 80\n",
    "    [ ] 10\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3c711-bb2e-401a-85ab-f4eeaae84c99",
   "metadata": {},
   "source": [
    "## Q10\n",
    "---\n",
    "\n",
    "In Lecture 7 we predicted the sentiment of movie reviews. What is the main problem with the following statement:\n",
    "\n",
    "_\"The most positive movie review in the dataset is the review corresponding to the largest coefficient of our logistic regression classifier.\"_\n",
    "\n",
    "?\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "> We have one coefficient per feature, not per review. It should say _\"The most positive movie review in the dataset is the review corresponding to the largest `predict_proba` score for the positive class from our logistic regression classifier.\"_\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d659393-de84-40f0-9aaf-fe5c5d3fcfcf",
   "metadata": {},
   "source": [
    "## Q11\n",
    "---\n",
    "In Lecture 8 we covered the idea of overfitting on the validation set. We showed that the validation score might not lead you to the hyperparameters that actually give the best test score. In that case, if cross-validation scores are not a faithful representation of test scores, why not just use the test set directly to tune your hyperparameters?\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "> That would violate the Golden Rule and would introduce the exact problem that we previously had with cross-validation (well, probably worse because it's only one set and not cross-validation). And then we'd have no unseen test set left.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3997a53-7431-4d24-a71e-b7d719f916ad",
   "metadata": {},
   "source": [
    "## Q12\n",
    "---\n",
    "In hw2 you looked at histograms of different Spotify song features, separated by the target class (whether the user liked the song or not). Here is the histogram for danceability: \n",
    "\n",
    "![](img/danceability.png)\n",
    "\n",
    "In hw2 we used a decision tree classifier. If we were instead to try a logistic regression classifier on this dataset, and if we used danceability as our only feature, would you expect the logistic regression coefficient to be positive or negative? Briefly explain your reasoning.\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "> I would expect the coefficient to be positive because the positive class tends to have larger values of danceability than the negative class.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49c1ec-4cd4-490a-b4f1-dc90b5cd1ed9",
   "metadata": {},
   "source": [
    "## Q13\n",
    "---\n",
    "Suppose we working on the problem of classifying toxic comments. You are interested in reducing the number of false negatives. Which of the following metric should you primarily be trying to improve?\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "\n",
    "    [ ] precision\n",
    "    [X] recall\n",
    "    [ ] accuracy\n",
    "    [ ] MAPE\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa5b63-0bee-4c37-92b2-12fe57754dae",
   "metadata": {},
   "source": [
    "## Q14\n",
    "---\n",
    "In Lecture 9 we talked about different scoring metrics for binary classification. Let's say you train two classifiers on a given dataset. If the two classifiers have the same precision as each other, and the same recall as each other, does that mean they must have the same accuracy? Briefly explain.\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "> Almost but not quite - this can be avoided if the number of true positives is zero. Here is a counterexample.\n",
    ">\n",
    "> <u>Classifier 1</u>\n",
    ">\n",
    "> | negative | positive |\n",
    "> |:--------:|:--------:|\n",
    "> |     2    |     3    |\n",
    "> |     2    |     0    |\n",
    ">\n",
    "> <u>Classifier 2</u>\n",
    ">\n",
    "> | negative | positive |\n",
    "> |:--------:|:--------:|\n",
    "> |     1    |     4    |\n",
    "> |     2    |     0    |\n",
    ">\n",
    "> Both classifiers have zero precision and zero recall, but Classifier 1 has accuracy 2/7 and Classifier 2 has accuracy 1/7.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51089bfa-babb-44c6-af3f-48e69bf5718f",
   "metadata": {},
   "source": [
    "## Q15\n",
    "\n",
    "----\n",
    "In Lecture 10 we talked about scoring metrics for regression. I argued that RMSE (root mean squared error) and MAPE (mean absolute percent error) are more relatable or \"human-readable\" metrics than MSE (mean squared error). Why? \n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "> In regression the target has units. MSE has these units squared, which is hard to interpret. On the other hand RMSE has the original units and MAPE is unitless.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c23312-7457-437a-a7f3-1e951fbca8bf",
   "metadata": {},
   "source": [
    "## Q16\n",
    "---\n",
    "In Lecture 11 we discussed two types of ensemble methods, averaging and stacking. What is one advantage and one disadvantage of averaging over stacking? Max one advantage and one disadvantage (please do not list more).\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "> - Possible advantages of averaging over stacking: code is faster to run, simpler.\n",
    "> - Possible disadvantages of averaging over stacking: you have to trust all models equally even if some are better than others.\n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a921f73-e589-43f3-8f94-714fdf270bc6",
   "metadata": {},
   "source": [
    "## Q17\n",
    "---\n",
    "Suppose you are predicting song popularity (between 0 and 100) based on a bunch of numeric and ordinal features using `Ridge` model which learned a coefficient of 10.0 for the ordinal feature `speed` which has three categories in the order given below:\n",
    "\n",
    "```['slow', 'medium', 'fast']```\n",
    "\n",
    "Suppose for a test example with \"medium\" speed, the model predicts a popularity of 50 likes. If we change the value of the feature from \"medium\" to \"fast\", what would be the popularity prediction of the model? Briefly explain. \n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "> The new predicted popularity would be 60.\n",
    ">\n",
    "> This is because the learned coefficient of 10 can be interpreted as follows: increasing by one category of ord1 (e.g. medium -> fast) increases the predicted popularity by 10. \n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c607e9-6fc3-4e1b-b3d1-faa2051866a1",
   "metadata": {},
   "source": [
    "## Q18\n",
    "---\n",
    "Here is a SHAP dependence plot for the \"energy\" feature for SHAP values for class 1 (class 0: the user dislikes the song, class 1: the user likes the song) for the Spotify data set, which you used in homework 3. Comment on the relationship between the \"energy\" feature and the target class 1 based on this plot. \n",
    "\n",
    "![](img/shap_dependence_plot_class1.png)\n",
    "\n",
    "<b>BEGIN SOLUTION</b>\n",
    "> The plot captures the non-linear relationship between energy and the target to some extent. Based on this model, the user likes songs with middle range of energy, really dislikes songs with lower energy, and kind of dislikes songs with very high energy. \n",
    "\n",
    "<b>END SOLUTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5d656-6fab-4be1-9bbf-6adeb7dda964",
   "metadata": {},
   "source": [
    "## Q19\n",
    "---\n",
    "\n",
    "Assuming that all the appropriate libraries are imported, the code below tries to carry out hyperparameter optimization using random search. But the code below has some problems. Point out at least two major problems in this code, clearly mention the problems, and fix the code.\n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n",
    "vec = CountVectorizer()\n",
    "X_train_enc = vec.fit_transform(X_train)\n",
    "X_test_enc = vec.transform(X_test)\n",
    "lr = LogisticRegression(max_iter=2000)\n",
    "vocab = vec.get_feature_names_out()\n",
    "\n",
    "param_dist = {\n",
    "    \"logisticregression__C\": loguniform(1e-3, 1e3),\n",
    "    \"countvectorizer__max_features\": randint(100, len(vocab))\n",
    "}\n",
    "random_search = RandomizedSearchCV(\n",
    "    lr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_enc, y_train)\n",
    "```\n",
    "\n",
    "<b>BEGIN SOLUTION<b>\n",
    "\n",
    "Pointing out any of the two below should be fine.\n",
    "\n",
    "1. We are passing transformed data to random search, which has inbuilt cross-validation. We'll be breaking the golden rule when we carry out cross validation in random search.\n",
    "\n",
    "2. Since we are passing logistic regression directly to random search and we are not using pipelines, we do not need __ syntax.\n",
    "\n",
    "3. We are transforming the data outside random search. So `max_features` hyperparameter will be invalid inside the random search.\n",
    "\n",
    "Here is the fixed code.\n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)\n",
    "pipe_lr = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=2000))\n",
    "vocab = vec.get_feature_names_out()\n",
    "param_dist = {\n",
    "    \"logisticregression__C\": loguniform(1e-3, 1e3),\n",
    "    \"countvectorizer__max_features\": randint(100, len(vocab))\n",
    "}\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_lr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "```\n",
    "<b>END SOLUTION<b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789458ab-cbe0-443f-8897-ab1ec30fb51b",
   "metadata": {},
   "source": [
    "## Q20\n",
    "---\n",
    "In Lecture 8 we talked about optimization bias or overfitting of the validation set. Why are small datasets more prone to the phenomenon of overfitting the validation set? Briefly explain. \n",
    "\n",
    "<b>BEGIN SOLUTION<b>\n",
    "The validation error is noisier with less data. A smaller dataset means the validation splits are going to be small during cross-validation of hyperparameter optimization. If we try out many possibilities on this small validation set, we might get good scores by luck for a certain hyperparameter combination which do not generalize well on the test set or the deployment data. \n",
    "<b>END SOLUTION<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5f985-1878-4ba1-87ef-5c43c0eb9795",
   "metadata": {},
   "source": [
    "## Q21\n",
    "---\n",
    "In hw8 you worked with text features generated from CountVectorizer vs. text features generated from pre-trained word embeddings. Let's say you had the following two texts:\n",
    "\n",
    "\"Why do u study all the time, can't we hang out @ the pub instead instead?\"\n",
    "\n",
    "and\n",
    "\n",
    "\"You're always researching and working instead of meeting up with me at the bar!\"\n",
    "\n",
    "Which of the two approaches, CountVectorizer or pre-trained word embeddings, seems more promising for detecting that these two messages are similar? Briefly justify your answer.\n",
    "\n",
    "<b>BEGIN SOLUTION<b>\n",
    "\n",
    "I would say the pre-trained embedding, because it can tell that there are similar words, like study vs. researching/working, or pub vs. bar. On the other hand, CountVectorizer only looks for the exact same words being present in both messages, and there's not a lot of overlap here.\n",
    "\n",
    "<b>END SOLUTION<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b0fe8-c8fb-43b0-abde-55eab27ae8a3",
   "metadata": {},
   "source": [
    "Suppose you run K-Means twice with different initializations on a toy dataset and you get the following cluster assignments with the two runs: K-Means run A and K-Means run B, as shown below. \n",
    "\n",
    "|           | K-Means run A  | K-Means run B |\n",
    "| --------- | --------- | --------- |\n",
    "| example 1 |\t0 \t         | 2 |\n",
    "| example 2 |\t1 \t         | 1 |\n",
    "| example 3 |\t1 \t         | 1 |\n",
    "| example 4 |\t0 \t         | 2 |\n",
    "| example 5 |\t0 \t         | 2 |\n",
    "| example 6 |\t2 \t         | 0 |\n",
    "\n",
    "Are the two runs resulting in the same set of cluster centers? Briefly explain. (Max 60 words.)  \n",
    "\n",
    "<b>BEGIN SOLUTION<b>\n",
    "\n",
    "Although the labels are quite different, the clusters are equivalent and they will result in the same set of cluster centers. \n",
    "\n",
    "<b>END SOLUTION<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0df023-a3c5-4d14-9224-5cfb2a40f6a2",
   "metadata": {},
   "source": [
    "## Q22\n",
    "---\n",
    "\n",
    "Continuing the previous question, suppose K-Means run A returns the following cluster centers. \n",
    "| | | |\n",
    "| ---- | ---- | --- |\n",
    "| center 0\t| 5.5\t| 1.7 |\n",
    "| center 1\t| -2.6\t| 9.1 |\n",
    "| center 2\t| 1.0\t| -1.0| \n",
    "\n",
    "What might be d (the number of features) in this data?  \n",
    "\n",
    "<b>BEGIN SOLUTION<b>\n",
    "- [X] 2\n",
    "- [ ] 3\n",
    "- [ ] 4\n",
    "- [ ] Cannot determine from the given information \n",
    "<b>END SOLUTION<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7961a-8ca4-44a8-b3dd-c39d89ff27a1",
   "metadata": {},
   "source": [
    "## Q23\n",
    "---\n",
    "\n",
    "Continuing the previous question: To which cluster would K-Means run A assign the new point [0, 0]? \n",
    "\n",
    "<b>BEGIN SOLUTION<b>\n",
    "- [ ] Center 0\n",
    "- [ ] Center 1\n",
    "- [X] Center 2\n",
    "- [ ] Cannot determine from the given information\n",
    "<b>END SOLUTION<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e425c-48e1-434f-b91b-fd24d05e95aa",
   "metadata": {},
   "source": [
    "## Q24\n",
    "---\n",
    "\n",
    "Continuing the previous question: suppose you fit DBSCAN model on this toy dataset. Would you be able to call predict on the new point [0, 0]? Why or why not?  \n",
    "\n",
    "<b>BEGIN SOLUTION<b>\n",
    "\n",
    "No. Unlike K-Means, DBSCAN doesn't have a clear notion of predict for new points. To get a cluster assignment for a new point, you have to run the algorithm again by including this point. \n",
    "\n",
    "<b>END SOLUTION<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789da4fb-9aac-4c54-b0e7-c07dc89a74ec",
   "metadata": {},
   "source": [
    "## Q25\n",
    "---\n",
    "\n",
    "Let's imagine a company is building a supervised machine learning system to predict the success of future employees, to be used in the hiring process. The features for each employee include some CountVectorizer features from their resume & cover letter, pre-trained word embedding features based on the person's full name, some educational features (level of education, GPA), some professional experience features (years of work experience, categorical features based on past job titles), etc. They train the model using data from existing employees, with the target variable set as the current salary of those current employees, which is taken as a proxy for success. Describe a major issue with this approach from a bias/fairness perspective.\n",
    "\n",
    "<b>BEGIN SOLUTION<b>\n",
    "\n",
    "The algorithm will learn to emulate a system that is already biased. Salaries are probably not a good metric because it is known that there are wage gaps, for example between men and women. Furthermore, some of the features used are problematic as well, such as using their name, which can be tied to gender, race and culture. There likely other issues as well.\n",
    "\n",
    "<b>END SOLUTION<b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "cpsc330"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
